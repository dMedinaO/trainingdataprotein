'''
clase con la responsabilidad de generar la ejecuci√≥n de los mejores modelos obtenidos en la ejecucion full data y obtener
todos los posibles resultados para un clasificador
'''
from scipy import interp
from itertools import cycle
import itertools
import numpy as np
import subprocess
import matplotlib.pyplot as plt

from sklearn.metrics import fbeta_score, make_scorer, confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, precision_recall_curve, average_precision_score
from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, precision_score, recall_score, log_loss, hamming_loss
from sklearn.model_selection import cross_validate, cross_val_predict, cross_val_score, KFold, StratifiedKFold, validation_curve, learning_curve, train_test_split, ShuffleSplit, LeaveOneOut
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from proyect.CCProcesFile import document
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import NuSVC
from proyect.CCTraining.LOU import performance
from proyect.CCTraining.LOU import performanceScoreValues

class createBestModels(object):

    def __init__(self, dataSet, pathOutput):

        self.matrix = dataSet
        self.pathOutput = pathOutput
        self.ListScore = ['accuracy', 'recall', 'precision']
        self.ftwo_scorer = make_scorer(fbeta_score, beta=2)
        self.classAttribute = []
        self.dataWC = []
        self.prepareDataSet()
        self.performaceObject = performance.performanceTraining()#para almacenar las performance...

    #metodo que permite procesar los modelos, crea los clf y genera los resultados
    def processModels(self):

        print "MLPClassifier logistic-sgd-invscaling (10-10-15)"
        clf = MLPClassifier(hidden_layer_sizes=(10,10,15), activation='logistic', solver='sgd', learning_rate='invscaling')
        param_name = "alpha"
        param_range = np.logspace(-6, -1, 5)
        self.createModel(clf, "MLPClassifier_invscaling_logistinc_sgd_10-10-15/", 0.2, param_name, param_range, 'alpha', 'Validation curve with MLPClassifier', 0)


    #metodo que permite crear un directorio...
    def createPath(self, namePath):

        namePath = self.pathOutput+namePath
        command = "mkdir -p %s" % namePath
        subprocess.call(command, shell=True)

        return namePath

    #metodo que permite hacer la lectura y la preparacion del set de datos...
    def prepareDataSet(self):

        for element in self.matrix:
            self.dataWC.append(element[:-1])
            self.classAttribute.append(element[-1])

    #crear el modelo para gradientes...
    def createModel(self, clf, pathName, size, param_name, param_range, xlabel, title, index):

        path = self.createPath(pathName)
        clfNull = clf
        clf = clf.fit(self.dataWC, self.classAttribute)

        print "Process summary score"
        self.crossValidateModel(clf, path)

        # print "Process confusion matrix"
        # nameMatrix = "%sConfusionMatrix.svg" % path
        # self.createConfusionMatrix(self.dataWC, self.classAttribute, clf, nameMatrix)
        # try:
        #     print "Process Precision and recall curve"
        #     fig1 = "%sprecision_recall_curve.svg" % path
        #     fig2 = "%sprecision_recall_curve_for_class.svg" % path
        #     self.plot_precision_and_recall_curve(clfNull, fig1, fig2)
        # except:
        #     print "Not precision_recall_curve"
        #     pass
        #
        # try:
        #     print "Process ROC curve"
        #     namePicture = "%sroc_curve.svg" % path
        #     self.createCurveRoc(clfNull, namePicture)
        # except:
        #     print "Not roc_curve"
        #     pass
        #
        # try:
        #     print "Process validation curve"
        #     namePicture = "%svalidation_curve.svg" % path
        #     self.createCurveValidation(clfNull, namePicture, param_name, param_range, xlabel, title, index)
        # except:
        #     print "Not validation_curve"
        #     pass
        #
        # try:
        #     print "Process learning curve"
        #     namePicture = "%slearning_curve.svg" % path
        #     self.createLearningCurve(clfNull, namePicture, "Learning Curve For GradientBoostingClassifier", size)
        # except:
        #     print "It is not possible create learning curve"
        #     pass

    def transformInt(self, listData):
        for i in range(len(listData)):
            listData[i] = int(listData[i])
        return listData

    def plot_precision_and_recall(self, random_forest, name):
        y_scores = random_forest.predict_proba(self.dataWC)
        y_scores = y_scores[:,1]
        self.y_scores = y_scores

        precision, recall, threshold = precision_recall_curve(self.transformInt(self.classAttribute), y_scores)
        plt.figure()
        plt.plot(threshold, precision[:-1], "r-", label="precision", linewidth=5)
        plt.plot(threshold, recall[:-1], "b", label="recall", linewidth=5)
        plt.xlabel("threshold", fontsize=19)
        plt.legend(loc="upper right", fontsize=19)
        plt.ylim([0, 1])
        plt.savefig(name)

    #generamos la validacion del modelo...
    def crossValidateModel(self, clf, path):

        accuracy = []
        precision = []
        recall = []
        for i in range(100):

            loocv = LeaveOneOut()
            scores = cross_val_score(clf, self.dataWC, self.classAttribute, cv=loocv, scoring='accuracy')
            accuracy.append(scores.mean())

            scores = cross_val_score(clf, self.dataWC, self.classAttribute, cv=loocv, scoring='precision')
            precision.append(scores.mean())

            scores = cross_val_score(clf, self.dataWC, self.classAttribute, cv=loocv, scoring='recall')
            recall.append(scores.mean())

        print np.mean(accuracy)
        print np.mean(precision)
        print np.mean(recall)

        #exportamos la data...
        #header = ["Metric", "Score"]
        #matrixData = [['accuracy', scoreData[0]], ['recall', scoreData[1]], ['precision', scoreData[2]], ['neg_log_loss', scoreData[3]], ['f1', scoreData[4]]]
        #document.document("summaryScore.csv", path).createExportFileWithPandas(matrixData, header)

    #desarrollo del plot de la matriz de confusion...
    def plot_confusion_matrix(self, cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Oranges):

        """
        This function prints and plots the confusion matrix.
        Normalization can be applied by setting `normalize=True`.
        """
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            print("Normalized confusion matrix")
        else:
            print('Confusion matrix, without normalization')

        print(cm)

        plt.imshow(cm, interpolation='nearest', cmap=cmap)
        plt.title(title)
        plt.colorbar()
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=45)
        plt.yticks(tick_marks, classes)

        fmt = '.2f' if normalize else 'd'
        thresh = cm.max() / 2.
        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
            plt.text(j, i, format(cm[i, j], fmt),
                     horizontalalignment="center",
                     color="black")

        plt.tight_layout()
        plt.ylabel('True label')
        plt.xlabel('Predicted label')

    #metodo que permite generar la matriz de confusion...
    def createConfusionMatrix(self, xTrain, yTrain, clf, nameFig):

        self.predictions = cross_val_predict(clf, xTrain, yTrain, cv=10)
        matrix = confusion_matrix(yTrain, self.predictions)

        np.set_printoptions(precision=2)

        # Plot non-normalized confusion matrix
        plt.figure()
        self.plot_confusion_matrix(matrix, classes=['Clinical','No clinical'], title='Confusion matrix, without normalization')
        plt.savefig(nameFig)

    #metodo que permite crear una curva roc con la data...
    def createCurveRoc(self, clf, nameFig):

        #recibimos la data y la transformamos en
        X = np.array(self.dataWC)
        y = np.array(self.transformInt(self.classAttribute))

        cv = StratifiedKFold(n_splits=10)
        classifier = clf
        tprs = []
        aucs = []
        mean_fpr = np.linspace(0, 1, 100)
        plt.figure(figsize=(20,10))
        i = 0
        for train, test in cv.split(X, y):
            probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])
            # Compute ROC curve and area the curve
            fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
            tprs.append(interp(mean_fpr, fpr, tpr))
            tprs[-1][0] = 0.0
            roc_auc = auc(fpr, tpr)
            aucs.append(roc_auc)
            plt.plot(fpr, tpr, lw=1, alpha=0.3,
                     label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))

            i += 1
        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
                 label='Luck', alpha=.8)

        mean_tpr = np.mean(tprs, axis=0)
        mean_tpr[-1] = 1.0
        mean_auc = auc(mean_fpr, mean_tpr)
        std_auc = np.std(aucs)
        plt.plot(mean_fpr, mean_tpr, color='b',
                 label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
                 lw=2, alpha=.8)

        std_tpr = np.std(tprs, axis=0)
        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,
                         label=r'$\pm$ 1 std. dev.')

        plt.xlim([-0.05, 1.05])
        plt.ylim([-0.05, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver operating characteristic example', fontsize=25)
        plt.legend(loc="lower right")
        plt.savefig(nameFig)

    #metodo que permite crear la curva de validacion
    def createCurveValidation(self, clf, nameFig, param_name, param_range, xlabel, title, option):

        X = np.array(self.dataWC)
        y = np.array(self.transformInt(self.classAttribute))

        train_scores, test_scores = validation_curve(clf, X, y, param_name=param_name, param_range=param_range,cv=10, scoring="accuracy", n_jobs=1)
        train_scores_mean = np.mean(train_scores, axis=1)
        train_scores_std = np.std(train_scores, axis=1)
        test_scores_mean = np.mean(test_scores, axis=1)
        test_scores_std = np.std(test_scores, axis=1)

        plt.figure()
        plt.title(title)
        plt.xlabel(xlabel)
        plt.ylabel("Score")
        plt.ylim(0.0, 1.1)
        lw = 2
        #si grafica en forma de logaritmo o no...
        if option == 1:
            plt.plot(param_range, train_scores_mean, label="Training score",color="darkorange", lw=lw)
            plt.fill_between(param_range, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.2,color="darkorange", lw=lw)
            plt.plot(param_range, test_scores_mean, label="Cross-validation score",color="navy", lw=lw)
            plt.fill_between(param_range, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.2,color="navy", lw=lw)
        else:
            plt.semilogx(param_range, train_scores_mean, label="Training score",color="darkorange", lw=lw)
            plt.fill_between(param_range, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.2,color="darkorange", lw=lw)
            plt.semilogx(param_range, test_scores_mean, label="Cross-validation score",color="navy", lw=lw)
            plt.fill_between(param_range, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.2,color="navy", lw=lw)
        plt.legend(loc="best")
        plt.savefig(nameFig)

    #metodo que permite crear la curva de aprendizaje...
    def plot_learning_curve(self, estimator, title, X, y, cv, ylim=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):
        plt.figure()
        plt.title(title)
        if ylim is not None:
            plt.ylim(*ylim)
        plt.xlabel("Training examples")
        plt.ylabel("Score")
        train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)

        train_scores_mean = np.mean(train_scores, axis=1)
        train_scores_std = np.std(train_scores, axis=1)
        test_scores_mean = np.mean(test_scores, axis=1)
        test_scores_std = np.std(test_scores, axis=1)
        plt.grid()

        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                         train_scores_mean + train_scores_std, alpha=0.1,
                         color="r")
        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1, color="g")
        plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
                 label="Training score")
        plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
                 label="Cross-validation score")

        plt.legend(loc="best")
        return plt

    def createLearningCurve(self, clf, nameFig, title, size):

        X = np.array(self.dataWC)
        y = np.array(self.transformInt(self.classAttribute))

        # Cross validation with 100 iterations to get smoother mean test and train
        # score curves, each time with 20% data randomly selected as a validation set.
        cv = ShuffleSplit(n_splits=100, test_size=size, random_state=0)
        self.plot_learning_curve(clf, title, X, y, cv, ylim=(0.01, 1.01), n_jobs=4)

        plt.savefig(nameFig)

    #metodo que permite aplicar las diversas curvas de precision vs recall
    def plot_precision_and_recall_curve(self, classifier, fig1, fig2):

        X = np.array(self.dataWC)
        y = np.array(self.transformInt(self.classAttribute))

        # Limit to the two first classes, and split into training and test
        X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],test_size=.5)

        # Create a simple classifier
        classifier.fit(X_train, y_train)
        y_score = classifier.decision_function(X_test)
        average_precision = average_precision_score(y_test, y_score)

        print('Average precision-recall score: {0:0.2f}'.format(average_precision))

        precision, recall, _ = precision_recall_curve(y_test, y_score)

        plt.figure()
        plt.step(recall, precision, color='b', alpha=0.2,where='post')
        plt.fill_between(recall, precision, step='post', alpha=0.2,color='b')

        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.ylim([0.0, 1.05])
        plt.xlim([0.0, 1.0])
        plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))

        plt.savefig(fig1)
